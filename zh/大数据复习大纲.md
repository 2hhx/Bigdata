## <center>大数据复习大纲</center> ##
> A卷复习提纲
# 填空题 #

- **分布式系统设计策略包括<font color="#dd0000">重试机制，心跳机制 ，副本机制</font>**
- **hadoop核心组件由<font color="#dd0000">HDFS，MapReduce，Yarn</font>这三大部分组成**
- **分布式系统数据分布设计包括<font color="#dd0000">哈希取模，一致性哈希，数据范围划分，数据库划分</font>**
- **分布式系统分布式协议包括<font color="#dd0000">Paxos机制，Lease机制</font>**
- **HDFS集群启动的时候，运行着<font color="#dd0000">namenode，datanode，secondaryNamenode</font>三个重要角色**
- **<font color="#dd0000">namenode</font>是HDFS架构中的主节点，用于维护和管理<font color="#dd0000">DataNode</font>上存储的block块**
- **强制Namenode进入safe Mode模式的命令是<font color="#dd0000">hdfs dfsadmin –safemode enter</font>**
- **yarn自带的资源调度器有<font color="#dd0000">FIFO ,  Capacity Scheduler ,  Fair Scheduler</font>**
- **Zookeeper是一个分布式开源的应用程序<font color="#dd0000">协调服务</font>，其中提供的服务包括 <font color="#dd0000">配置维护 ， 域名服务 ， 分布式锁</font>**
- **Zookeeper特性包括<font color="#dd0000">一致性，原子性，单一视图，可靠性，实时性</font>**
- **启动zookeeper命令<font color="#dd0000">zkServer.sh start</font>，启动zookeeper客户端命令<font color="#dd0000">zkCli.sh</font>，暂停命令为<font color="#dd0000">zkServer.sh stop</font>**
- **Hbase数据存储在<font color="#dd0000">HDFS</font>之上**

---
# 选择题 #
- **Hadoop1.0默认的调度器策略是<font color="#dd0000">先进先出调度器</font>**
- **MapReduce擅长哪个领域的计算是<font color="#dd0000">离线计算</font>**
- **一个标准的生产环境中Zookeeper实例个数<font color="#dd0000">不可能是偶数台机器</font>**
- **Hive不支持所有的标准SQL语法，<font color="#dd0000">Hive的HQL语法和SQL语法不完全一致</font>**
- **Hbase中的术语有<font color="#dd0000">Column，Column Family，Row Key，Meta</font>**
- **<font color="#dd0000">Flume</font>的主要作用是<font color="#dd0000">数据采集</font>**
- **Hbase支持多语言（比如C++，python等）访问，为实现该功能，它采用的开源软件是<font color="#dd0000">thrift</font>**
- **Zookeeper集群中<font color="#dd0000">Leader服务器在整个运行期间有且仅有一台</font>，<font color="#dd0000">PERSISTENT_SEQUENTIAL是永久有序节点类型</font>**
- **在<font color="#dd0000">hadoop HA</font>中，HDFS由2个namenode组成，一个leader一个follower**

---

**数仓对象不是单一化的【需要将数仓特点背出来】**
<font color="#dd0000">

- **主题性**
- **集成性**
- **稳定性**
- **动态性**

</font>

**(5分)zookeeper的znode有4种类型，分别有**
<font color="#dd0000">

- **(1) 持久化节点。**
- **(2) 顺序持久化节点。**
- **(3) 临时节点。**
- **(4) 顺序临时节点**。  

</font>

---

# 判断题√ #
- **<font color="#dd0000">Hadoop不支持随机读写</font>**。
- **<font color="#dd0000">MapReduce的input split不一定等于一个block</font>**
- **MpaReduce适用于处理PB级别的<font color="#dd0000">离线业务数据</font>**
- **Hive将<font color="#dd0000">元数据</font>存储在数据库中**
- **<font color="#dd0000">Hadoop HDFS为Hive提供了高可靠性的底层存储支持</font>**
- **<font color="#dd0000">数据采集可以通过flume框架来完成</font>**
- **<font color="#dd0000">ETL</font>包括<font color="#dd0000">数据抽取，数据交互转换和数据加载</font>过程**
- **<font color="#dd0000">数仓</font>特点包括<font color="#dd0000">数据源多样化，数据量大和服务对象多样化</font>**
- **<font color="#dd0000">MongoDB</font>是一个<font color="#dd0000">高性能，开源，无模式的文档型数据库</font>**
- **在Zookeeper中，<font color="#dd0000">znode创建的类型有4种类型</font>**
- **hive的查询语言  <font color="#dd0000">hql</font>**
- **hive<font color="#dd0000">元数据</font>存储在  <font color="#dd0000">数据库</font>**    
- **hive<font color="#dd0000">源数据</font>存储在  <font color="#dd0000">HDFS</font>**     
- **hive执行任务的是   mapReduce**

---

**HDFS核心组件有哪些**

<font color="#dd0000">

- (1)namenode。
- (2)datanode。
- (3)Secondarynamenode

</font>
---

#简答题#

**安装HDFS时候，需要修改几个重要的配置文件，这些重要的配置文件分别有：**
<font color="#dd0000">

- (1) slaves。
- (2) core-site.xml。
- (3) hdfs-site.xml。
- (4) hadoop-env.sh。

</font>

**安装一个最为简单的Hbase分布式集群，需要配置几个基本配置文件，分别有：**
<font color="#dd0000">

- (1)  hbase-env.sh  
- (2)  hbase-site.xml  
- (3)  regionservers 

</font>

**Hive和RDBMS对比，Hive特点：**
<font color="#dd0000">

- a.查询语言HQL
- b.数据存储HDFS
- c.执行MapReduce
- d.执行延迟高
- e.处理数据规模大  
</font>

> B卷复习提纲

#填空题#

- 在Zookeeper当中，通过ZAB来构建高可用的<font color="#dd0000">分布式数据主备系统</font> ，而<font color="#dd0000">paxos</font>是用来构建<font color="#dd0000">分布式一致性状态机系统 </font>
- <font color="#dd0000">Hadoop生态系统组件</font>非常之多，比如<font color="#0000dd">=></font><font color="#dd0000">HDFS,MapReduce,Hbase,Zookeeper,Hive ,storm,spark，flume</font>等
- Hive支持用户<font color="#dd0000">自定义函数</font>，用可以根据自己的需求来实现自己的函数。
- Hbase是基于hadoop的一个<font color="#dd0000">分布式NoSQL数据库</font>。
- <font color="#dd0000">数据采集</font>可以通过<font color="#dd0000">flume</font>框架来实现。
- Hive具有<font color="#dd0000">可扩展，延展性，容错</font>等点
- Hive架构图中基本组成包括<font color="#dd0000">用户接口，元数据存储，解析器，编译器，优化器和执行器等</font>
- Hadoop中， Client端将文件切分为<font color="#dd0000">Block</font>,依次上传
- HBase是一个分布式<font color="#dd0000">面向列</font>的<font color="#dd0000">开源数据库</font>
- zookeeper是<font color="#dd0000">不适合做队列</font>的，由于zookeeper有<font color="#dd0000">1MB传输限制</font>，存在过多的节点会导致zookeeper启动非常慢，zookeeper<font color="#dd0000">数据完全存储在内存，大量的队列意味着占用很多内存</font>
- <font color="#dd0000">数据仓库</font>主要应用于<font color="#dd0000">OLAP(联机分析处理)</font>
- SQL数据存储<font color="#dd0000">特定结构表</font>中
- <font color="#dd0000">Hive属于ETL工具,数据存储于HDFS中,</font>

#选择题和判断题#

- <font color="#dd0000">SecondaryNamenode</font>它在HDFS执行常规的检查点
- <font color="#0000dd">表名信息</font><font color="#dd0000">不会存储在Hbase的一个cell中</font>
- 在zookeeper中，<font color="#dd0000">持久节点下面支持创建子节点,临时节点不支持创建子节点</font>
- <font color="#dd0000">MapReduce是一个计算框架，可以运行在yarn上</font>
- <font color="#dd0000">Flume</font>的主要作用是<font color="#dd0000">数据采集</font>
- <font color="#dd0000">Zookeeper实例个数在生产环境</font>当中应该是<font color="#dd0000">奇数个</font>
- Hive底层采用的计算引擎是<font color="#dd0000">MapReduce</font>
- <font color="#dd0000">Block和split之间对应关系是任意的，可以由用户控制</font>
- Zookeeper采用<font color="#dd0000">递增事务ID</font>来保证事务顺序一致性。
- 在zookeeper集群当中，如果<font color="#dd0000">leader崩溃</font>或者<font color="#dd0000">失去大多数follower</font>，这个时候<font color="#dd0000">zookeeper进入恢复模式</font>。
- <font color="#dd0000">启动Hbase需要启动Hadoop集群</font>
- <font color="#dd0000">Hbase</font>是一个<font color="#dd0000">分布式的面向列的开源数据库</font>，是一个<font color="#dd0000">NoSQL数据库</font>
- <font color="#dd0000">Sqoop为Hbase</font>提供了方便的<font color="#dd0000">RDBMS数据导入</font>功能。


#简答题#

**分布式系统数据分布设计当中，主要包括：**
<font color="#dd0000">

- (1)   哈希取模    
- (2)   一致性哈希   
- (3)    数据范围划分   
- (4)    数据库划分

</font>

**Hbase相关命令：**
<font color="#dd0000">

- (1)创建表的关键字create。
- (2)描述表结构信息的关键字 describe 。
- (3)插入数据关键字   put。
- (4)获取数据关键字   get   
- (5)扫描表结构关键字  scan

</font>

**Haoop启动过程，namenode启动进度包括：**
<font color="#dd0000">

- (1) 读取fsimage。
- (2)  读取edit logs 。
- (3)  写入新的检查点  。   

</font>

> AB卷代码题(综合)

#使用hive创建表与加载数据#

	创建表:
	create table t_table(
	id int,
	name string,
	age int)
	row delimited formated
	fields terminated by "\t";

	加载数据:
	load data local inpath '/root/data.log' into table t_table

#ZooKeeper官方api的使用(CRUD)#

	public class ZKDemo {
	   private static final String CONNECT="192.168.74.90:2181,192.168.74.91:2181,192.168.74.92:2181";（2分）
	    private static final int SESSION_TIME_OUT=3000;
	    private ZooKeeper zk;
	    public ZKDemo(){
	       try {
	           zk=new ZooKeeper(CONNECT, SESSION_TIME_OUT, new Watcher() {
	               public void process(WatchedEvent watchedEvent) {
	                   
	               }
	           });
	       } catch (IOException e) {
	           e.printStackTrace();
	       }
	   }
	   public void rmr(String path)throws Exception{
	        List<String> children = zk.getChildren(path, false); 
	        if(children.size()!=0){
	            for(String child:children){
	              String newPath=path.concat("/"+child);
	                rmr(newPath);
	            }
	        }
	        System.out.println(path);
	        zk.delete(path,-1);
	    }
	}

#hive自定义UDF函数(一进一出)的使用#
	
	(1)自定义UDF函数
	public class CustomUpper extends UDF {
	    public String evaluate(String str){
	        return str.toUpperCase();
	    }
	}

	(2) add jar /root/custom.jar;   
	(3)create temporary function my_upper as 'com.example.hive.udf.CustomUpper ';
	(4) select my_upper(name) from t_hive; 

#向HBase表中添加数据#

	put 't1','r1','f1:info','{name:n1,data:1}'
	put 't1','r1','f2:ex_info_1','{name:n1,data:1}'
	put 't1','r2','f1:info','{name:n1,data:1}'
	put 't1','r3','f1:info','{name:n1,data:1}'
	put 't1','r5','f2:ex_info_2','{name:n1,data:1}'


